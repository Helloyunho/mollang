import { Lexer } from './tokenizer.ts'
import { ASTParser } from './ast.ts'

const lexer = new Lexer(`
몰??
모올??.?????

몰루
아모올루

모오올은?행
모올루
털!자

가?자!`)

const test = new Lexer(`몰????.??????????? 모올????.????.?? 모오올몰모올
아모오올!!!!루모오올모올아모오올!!!!!!!루아모오올루아모오올루
아모오올???루아몰루아모올루몰모올아몰???????????루아모오올???루
아몰모올??????루아모오올루아모오올!!!!!!!!루아모올?루`)
Deno.writeFile(
  'token.json',
  new TextEncoder().encode(JSON.stringify(test.tokens, null, 2))
)
const ast = new ASTParser(test.tokens)
Deno.writeFile(
  'ast.json',
  new TextEncoder().encode(JSON.stringify(ast.parseProgram(), null, 2))
)
